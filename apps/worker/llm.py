"""
LLM module for generating grounded explanations.

Uses OpenAI GPT to explain solutions based on:
1. The problem text
2. The answer from the database
3. The theoretical material from the section
"""
import os
import re
from typing import Optional

from openai import OpenAI

# Get API key from environment
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL_TEXT", "gpt-4o-mini")


SYSTEM_PROMPT = """–¢—ã ‚Äî —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±—ä—è—Å–Ω–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ —É—á–µ–Ω–∏–∫—É.

–ü–†–ê–í–ò–õ–ê:
1. –ü–æ —É—Å–ª–æ–≤–∏—é –∑–∞–¥–∞—á–∏ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏, –ß–¢–û —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞–π—Ç–∏ (–≤–µ–ª–∏—á–∏–Ω–∞, –æ–±—ä–µ–∫—Ç, –æ—Ç–Ω–æ—à–µ–Ω–∏–µ ‚Äî —á—Ç–æ —É–≥–æ–¥–Ω–æ –∏–∑ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏). –í —Å–≤–æ—ë–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–∏ —Ä–µ—à–∞–π —Å—Ç—Ä–æ–≥–æ —ç—Ç—É –∑–∞–¥–∞—á—É –∏ –Ω–µ –ø–æ–¥–º–µ–Ω—è–π –µ—ë –¥—Ä—É–≥–æ–π: –Ω–µ –º–µ–Ω—è–π –∏—Å–∫–æ–º—É—é –≤–µ–ª–∏—á–∏–Ω—É –Ω–∞ –¥—Ä—É–≥—É—é –∏ –Ω–µ –æ—Ç–≤–µ—á–∞–π –Ω–∞ –¥—Ä—É–≥–æ–π –≤–æ–ø—Ä–æ—Å.
2. –ò—Å–ø–æ–ª—å–∑—É–π –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —É—Å–ª–æ–≤–∏—è (–±—É–∫–≤—ã, —Å–∏–º–≤–æ–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–¥–∞—á–∏) –∏ –Ω–µ –≤–≤–æ–¥–∏ –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
3. –û–ø–∏—Ä–∞–π—Å—è –Ω–∞ —Ç–µ–æ—Ä–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ú–ê–¢–ï–†–ò–ê–õ–ê –†–ê–ó–î–ï–õ–ê, –µ—Å–ª–∏ –æ–Ω –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ —Å–º—ã—Å–ª—É.
4. –û–±—ä—è—Å–Ω—è–π –ø–æ—à–∞–≥–æ–≤–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ.
5. –ï—Å–ª–∏ –¥–∞–Ω –æ—Ç–≤–µ—Ç ‚Äî –ø–æ–∫–∞–∂–∏, –∫–∞–∫ –∫ –Ω–µ–º—É –ø—Ä–∏–π—Ç–∏; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –≤—ã–≤–µ–¥–∏ —Ñ–æ—Ä–º—É–ª—É –∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —É—Å–ª–æ–≤–∏—è.
6. –ò—Å–ø–æ–ª—å–∑—É–π –æ–±—ã—á–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è: ¬∞, ¬≤, ‚àö –∏ —Ç.–¥.
7. –û–±—ä—ë–º: –¥–æ 300 —Å–ª–æ–≤. –§–æ—Ä–º–∞—Ç: –∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∏ —à–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è.

–ï—Å–ª–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ —Ä–∞–∑–¥–µ–ª–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω –Ω–µ –ø–æ —Ç–µ–º–µ ‚Äî –æ–±—ä—è—Å–Ω–∏ –Ω–∞ –æ–±—â–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö, –Ω–æ –∏—Å–∫–æ–º–æ–µ –≤ —Ç–≤–æ—ë–º –æ—Ç–≤–µ—Ç–µ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ç–µ–º, —á—Ç–æ –ø—Ä–æ—Å—è—Ç –≤ —É—Å–ª–æ–≤–∏–∏."""


def _extract_requested_quantity(problem_text: str) -> Optional[str]:
    """–ò–∑ —É—Å–ª–æ–≤–∏—è –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É ¬´—á—Ç–æ –Ω–∞–π—Ç–∏¬ª (–ø–æ —à–∞–±–ª–æ–Ω–∞–º ¬´–ù–∞–π–¥–∏—Ç–µ ‚Ä¶¬ª, ¬´–ù–∞–π—Ç–∏ ‚Ä¶¬ª, ¬´–ß–µ–º—É —Ä–∞–≤–Ω–∞ ‚Ä¶¬ª)."""
    if not problem_text or len(problem_text.strip()) < 10:
        return None
    patterns = [
        r"–Ω–∞–π–¥–∏—Ç–µ\s+([^.,]+?)(?:\.|,|$)",
        r"–Ω–∞–π—Ç–∏\s+([^.,]+?)(?:\.|,|$)",
        r"—á–µ–º—É\s+—Ä–∞–≤–Ω[–∞–æ—ã]\s+([^.?]+)",
    ]
    text_lower = problem_text.strip().lower()
    for pattern in patterns:
        m = re.search(pattern, text_lower, re.IGNORECASE)
        if m:
            phrase = m.group(1).strip()
            if 3 < len(phrase) < 80:
                return phrase
    return None


def get_section_theory(db, book_id: int, section: str) -> str:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –¥–ª—è LLM (–æ—Ç–≤–µ—Ç—ã –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã,
    –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π). –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä—ë—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –ø—Ä–∏ ingestion section_theory,
    –∏–Ω–∞—á–µ ‚Äî —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ pdf_pages.ocr_text.
    """
    from sqlalchemy import text
    from models import SectionTheory

    if not section:
        return ""

    section_match = re.search(r"(\d+)", section)
    section_num = section_match.group(1) if section_match else None
    section_label = section if (section and section.strip().startswith("¬ß")) else (f"¬ß{section_num}" if section_num else "")

    # 1. –°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –ø—Ä–∏ ingestion —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª
    if section_label:
        row = db.query(SectionTheory).filter(
            SectionTheory.book_id == book_id,
            SectionTheory.section == section_label,
        ).first()
        if row and (row.theory_text or "").strip():
            return (row.theory_text or "").strip()[:8000]

    # 2. Fallback: —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (–¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è section_theory –≤ –ë–î)
    if not section_num:
        return ""

    result = db.execute(text("""
        SELECT pp.page_num, pp.ocr_text
        FROM pdf_pages pp
        JOIN pdf_sources ps ON ps.id = pp.pdf_source_id
        WHERE ps.book_id = :book_id
        ORDER BY pp.page_num
    """), {"book_id": book_id})

    theory_texts = []
    in_section = False
    pages_collected = 0
    max_pages = 3

    for row in result:
        ocr_text = row.ocr_text or ""
        section_header = re.search(rf"[¬ß$]\s*{section_num}[.,\s]", ocr_text[:500])
        if section_header:
            in_section = True
        elif in_section:
            new_section = re.search(r"[¬ß$]\s*(\d{1,2})[.,\s]", ocr_text[:300])
            if new_section and new_section.group(1) != section_num:
                break
        if in_section and pages_collected < max_pages:
            text_content = ocr_text
            exercises_start = len(text_content)
            for marker in ["–ó–∞–¥–∞—á–∏", "–£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è", "–ó–ê–î–ê–ß–ò", "–£–ü–†–ê–ñ–ù–ï–ù–ò–Ø", "–í–æ–ø—Ä–æ—Å—ã –¥–ª—è"]:
                pos = text_content.find(marker)
                if pos > 0 and pos < exercises_start:
                    exercises_start = pos
            theory_part = text_content[: min(exercises_start, 2500)]
            if len(theory_part) > 100:
                theory_texts.append(theory_part)
                pages_collected += 1

    return "\n\n".join(theory_texts)[:6000]


def generate_solution_explanation(
    problem_text: str,
    answer_text: Optional[str],
    section_theory: Optional[str],
    book_title: str = "",
    section: str = "",
) -> Optional[str]:
    """
    Generate an explanation of the solution using LLM.
    
    Args:
        problem_text: The problem/question text
        answer_text: The answer from the database (if available)
        section_theory: Theoretical material from the section
        book_title: Name of the textbook
        section: Section/paragraph number
        
    Returns:
        Explanation text or None if LLM call fails
    """
    if not OPENAI_API_KEY:
        print("‚ö†Ô∏è OPENAI_API_KEY not set, skipping LLM generation")
        return None
    
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        
        # Build the user prompt; optionally highlight what is asked
        requested = _extract_requested_quantity(problem_text)
        user_prompt = f"""–ó–ê–î–ê–ß–ê:
{problem_text}

"""
        if requested:
            user_prompt += f"""–í –£–°–õ–û–í–ò–ò –ü–†–û–°–Ø–¢ –ù–ê–ô–¢–ò: {requested}. –†–µ—à–∞–π –∏–º–µ–Ω–Ω–æ —ç—Ç—É –≤–µ–ª–∏—á–∏–Ω—É.

"""
        
        if answer_text:
            user_prompt += f"""–ò–ó–í–ï–°–¢–ù–´–ô –û–¢–í–ï–¢:
{answer_text}

"""
        
        if section_theory and len(section_theory) > 50:
            # Truncate if too long
            theory_truncated = section_theory[:4000]
            user_prompt += f"""–ú–ê–¢–ï–†–ò–ê–õ –†–ê–ó–î–ï–õ–ê {section}:
{theory_truncated}

"""
        
        user_prompt += """–ó–ê–î–ê–ù–ò–ï:
–ü–æ —É—Å–ª–æ–≤–∏—é –æ–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –ø—Ä–æ—Å—è—Ç –Ω–∞–π—Ç–∏. –î–∞–π –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∏ —Ä–µ—à–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–æ–π —Ü–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∑–∞–¥–∞—á–∏. –ò—Å–∫–æ–º–æ–µ –≤ —Ç–≤–æ—ë–º –æ—Ç–≤–µ—Ç–µ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–æ–π —É—Å–ª–æ–≤–∏—è."""
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=800,
        )
        
        explanation = response.choices[0].message.content
        
        # Track tokens used
        tokens_used = response.usage.total_tokens if response.usage else 0
        print(f"   ü§ñ LLM generated explanation ({tokens_used} tokens)")
        
        return explanation
        
    except Exception as e:
        print(f"‚ö†Ô∏è LLM generation failed: {e}")
        return None


def generate_quick_explanation(
    problem_text: str,
    answer_text: Optional[str],
) -> Optional[str]:
    """
    Generate a quick explanation without section theory (fallback).
    """
    if not OPENAI_API_KEY:
        return None
    
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        
        prompt = f"""–ó–∞–¥–∞—á–∞: {problem_text[:500]}
–û—Ç–≤–µ—Ç: {answer_text or '–Ω–µ –∏–∑–≤–µ—Å—Ç–µ–Ω'}

–ö—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏ —Ä–µ—à–µ–Ω–∏–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)."""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Use cheaper model for quick explanations
            messages=[
                {"role": "system", "content": "–¢—ã ‚Äî —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä. –ö—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω—è–π —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=300,
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"‚ö†Ô∏è Quick explanation failed: {e}")
        return None
