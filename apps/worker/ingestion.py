"""
PDF Ingestion Pipeline

–ü–∞–π–ø–ª–∞–π–Ω: OCR ‚Üí —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –≤ —Ñ–∞–π–ª—ã ‚Üí –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —Ñ–æ—Ä–º—É–ª/OCR ‚Üí –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª ‚Üí –∏–º–ø–æ—Ä—Ç –≤ –ë–î.
1. OCR (Tesseract) –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º ‚Üí –∑–∞–ø–∏—Å—å –≤ data/ocr_raw/{book_id}/{source_id}_{model}.md
2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (ocr_cleaner) –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º.
3. LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è (OpenAI): –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ OCR –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª –∫ —Ñ–æ—Ä–º–∞—Ç—É –¥–ª—è –ë–î/—á–∞—Ç–∞ (–±–µ–∑ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –∑–∞–º–µ–Ω).
4. –ó–∞–ø–∏—Å—å –≤ data/ocr_normalized/{book_id}/{source_id}.md –∏ –∏–º–ø–æ—Ä—Ç –≤ –ë–î (pdf_pages, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–¥–∞—á –∏ —Ç–µ–æ—Ä–∏–∏).

Usage:
    process_pdf_source(pdf_source_id=1)   # –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª OCR ‚Üí —Ñ–∞–π–ª—ã ‚Üí –ë–î
    import_from_normalized_file(pdf_source_id=1)  # –ø–µ—Ä–µ–∏–º–ø–æ—Ä—Ç –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –±–µ–∑ OCR
"""

import io
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

# PDF processing
try:
    import fitz  # pymupdf
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False
    print("‚ö†Ô∏è  pymupdf not installed")

# OCR: Tesseract only
try:
    import pytesseract
    from PIL import Image
    HAS_TESSERACT = True
except ImportError:
    HAS_TESSERACT = False
    print("‚ö†Ô∏è  pytesseract/Pillow not installed")

from config import get_settings
from database import SessionLocal

settings = get_settings()

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º —Å —Å—ã—Ä—ã–º —Ç–µ–∫—Å—Ç–æ–º)
try:
    from ocr_cleaner import clean_ocr_text
    HAS_OCR_CLEANER = True
except ImportError:
    HAS_OCR_CLEANER = False

    def clean_ocr_text(text: str, **kwargs) -> str:
        return text

# –§–∞–π–ª—ã OCR: —Å—ã—Ä–æ–π –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π .md –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
try:
    from ocr_files import write_raw_md, write_normalized_md, read_normalized_pages, get_ocr_normalized_path, get_llm_checkpoint_path
    HAS_OCR_FILES = True
except ImportError:
    HAS_OCR_FILES = False


# ===========================================
# Ingestion Job
# ===========================================

def process_pdf_source(pdf_source_id: int, local_pdf_path: Optional[str] = None) -> dict:
    """
    Process a PDF source: render pages, OCR, segment problems.
    
    Args:
        pdf_source_id: ID of PdfSource in database
        local_pdf_path: Optional path to local PDF file (if not using MinIO)
        
    Returns:
        dict with processing results
    """
    # Import models here to avoid circular imports
    from models import PdfSource, PdfPage, Problem, Book
    
    start_time = time.time()
    db = SessionLocal()
    
    try:
        # Get PDF source
        pdf_source = db.query(PdfSource).filter(PdfSource.id == pdf_source_id).first()
        if not pdf_source:
            return {"status": "error", "message": f"PdfSource {pdf_source_id} not found"}
        
        print(f"üìÑ Processing PDF source {pdf_source_id}: {pdf_source.original_filename}")
        
        # Update status
        pdf_source.status = "rendering"
        db.commit()
        
        # Get PDF data
        if local_pdf_path:
            pdf_path = local_pdf_path
        else:
            # TODO: Download from MinIO
            # For now, try to find locally based on minio_key
            base_path = Path(os.environ.get("DATA_DIR", "data"))
            pdf_path = base_path / pdf_source.minio_key
            
            if not pdf_path.exists():
                # Try alternate paths
                alt_paths = [
                    Path("data") / pdf_source.minio_key,
                    Path("..") / "data" / pdf_source.minio_key,
                ]
                for alt in alt_paths:
                    if alt.exists():
                        pdf_path = alt
                        break
        
        if not Path(pdf_path).exists():
            pdf_source.status = "failed"
            pdf_source.error_message = f"PDF file not found: {pdf_path}"
            db.commit()
            return {"status": "error", "message": f"PDF not found: {pdf_path}"}
        
        print(f"   üìÇ Loading from: {pdf_path}")
        
        # Process PDF
        if not HAS_PYMUPDF:
            return {"status": "error", "message": "pymupdf not installed"}
        
        doc = fitz.open(str(pdf_path))
        page_count = len(doc)
        pdf_source.page_count = page_count
        
        print(f"   üìÉ Found {page_count} pages")
        
        # Check if already being processed by another worker
        if pdf_source.status == "ocr":
            print(f"   ‚ö†Ô∏è  Already being processed, skipping")
            doc.close()
            return {"status": "skipped", "message": "Already being processed"}
        
        pdf_source.status = "ocr"
        db.commit()
        book_id = pdf_source.book_id
        
        # ‚Äî‚Äî 1. OCR –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (Tesseract), —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –≤ –ø–∞–º—è—Ç—å –∏ –≤ —Ñ–∞–π–ª ‚Äî‚Äî
        raw_texts = []
        ocr_confidences = []
        model_used = "tesseract"
        raw_path = norm_path = None
        if HAS_TESSERACT:
            print(f"   üì∑ OCR: Tesseract (rus+eng)")
        
        for page_num in range(page_count):
            page = doc[page_num]
            pix = page.get_pixmap(dpi=150)
            img_data = pix.tobytes("png")
            img = Image.open(io.BytesIO(img_data))
            text = ""
            conf = 70
            if HAS_TESSERACT:
                try:
                    text = pytesseract.image_to_string(img, lang="rus+eng")
                except Exception as e:
                    if page_num == 0:
                        print(f"   ‚ö†Ô∏è  OCR failed for page {page_num}: {e}")
            raw_texts.append(text or "")
            ocr_confidences.append(conf)
            # –ü—Ä–æ–≥—Ä–µ—Å—Å OCR –∫–∞–∂–¥—ã–µ 25 —Å—Ç—Ä–∞–Ω–∏—Ü
            if (page_num + 1) % 25 == 0 or page_num == page_count - 1:
                print(f"   üìÉ OCR: {page_num + 1}/{page_count} pages")
        
        doc.close()
        
        # –ó–∞–ø–∏—Å—å —Å—ã—Ä–æ–≥–æ OCR –≤ —Ñ–∞–π–ª
        if HAS_OCR_FILES and raw_texts:
            raw_path = write_raw_md(book_id, pdf_source_id, model_used, raw_texts)
            print(f"   üìÅ Raw OCR: {raw_path}")
        
        # ‚Äî‚Äî 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (ocr_cleaner) ‚Äî‚Äî
        normalized_texts = []
        for i, t in enumerate(raw_texts):
            if HAS_OCR_CLEANER and (t or "").strip():
                try:
                    normalized_texts.append(clean_ocr_text(t, use_dictionary=False))
                except Exception as e:
                    if i == 0:
                        print(f"   ‚ö†Ô∏è OCR clean skip for page 0: {e}")
                    normalized_texts.append(t or "")
            else:
                normalized_texts.append(t or "")

        # ‚Äî‚Äî 2b. LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —Ñ–æ—Ä–º—É–ª –∏ –æ—à–∏–±–æ–∫ OCR (OpenAI) ‚Äî‚Äî
        try:
            from llm_ocr_correct import correct_normalized_pages
            book = db.query(Book).filter(Book.id == book_id).first()
            subject = (book.subject if book else "geometry") or "geometry"
            print(f"   ü§ñ LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —Ñ–æ—Ä–º—É–ª/OCR (–ø—Ä–µ–¥–º–µ—Ç: {subject})...")
            normalized_texts = correct_normalized_pages(normalized_texts, subject=subject)
        except Exception as e:
            print(f"   ‚ö†Ô∏è LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞: {e}")

        # ‚Äî‚Äî 2c. –ó–∞–ø–∏—Å—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ ‚Äî‚Äî
        if HAS_OCR_FILES and normalized_texts:
            norm_path = write_normalized_md(book_id, pdf_source_id, normalized_texts)
            print(f"   üìÅ Normalized: {norm_path}")
        
        # ‚Äî‚Äî 3. –ò–º–ø–æ—Ä—Ç –≤ –ë–î: —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –∑–∞—Ç–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è ‚Äî‚Äî
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏—Ö –∑–∞–¥–∞—á–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        existing_pages = db.query(PdfPage).filter(PdfPage.pdf_source_id == pdf_source_id).all()
        for p in existing_pages:
            db.query(Problem).filter(Problem.source_page_id == p.id).delete()
        db.query(PdfPage).filter(PdfPage.pdf_source_id == pdf_source_id).delete()
        db.flush()
        
        pages_processed = 0
        problems_found = 0
        for page_num in range(page_count):
            text = normalized_texts[page_num] if page_num < len(normalized_texts) else ""
            conf = ocr_confidences[page_num] if page_num < len(ocr_confidences) else 70
            pdf_page = PdfPage(
                pdf_source_id=pdf_source_id,
                page_num=page_num,
                ocr_text=text,
                ocr_confidence=conf,
            )
            db.add(pdf_page)
            db.flush()
            problems = segment_problems(text, page_num)
            for prob in problems:
                problem = Problem(
                    book_id=book_id,
                    source_page_id=pdf_page.id,
                    number=prob.get("number"),
                    section=prob.get("section"),
                    problem_text=prob["text"],
                    solution_text=prob.get("solution_text"),
                    page_ref=f"—Å—Ç—Ä. {page_num + 1}",
                    confidence=prob.get("confidence", 50),
                )
                db.add(problem)
                problems_found += 1
            pages_processed += 1
            if pages_processed % 10 == 0:
                db.commit()
                print(f"   üìÉ Imported {pages_processed}/{page_count} pages, {problems_found} problems")
        
        theory_count = extract_and_save_section_theory(db, book_id, pdf_source_id)
        if theory_count is not None:
            print(f"   üìñ Section theory: {theory_count} paragraphs saved")
        
        pdf_source.status = "done"
        db.commit()
        elapsed = time.time() - start_time
        print(f"   ‚úÖ Done in {elapsed:.1f}s: {pages_processed} pages, {problems_found} problems")
        
        return {
            "status": "success",
            "pdf_source_id": pdf_source_id,
            "pages_processed": pages_processed,
            "problems_found": problems_found,
            "elapsed_seconds": elapsed,
            "raw_file": str(raw_path) if raw_path else None,
            "normalized_file": str(norm_path) if norm_path else None,
        }
        
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
        
        if pdf_source:
            pdf_source.status = "failed"
            pdf_source.error_message = str(e)
            db.commit()
        
        return {"status": "error", "message": str(e)}
    
    finally:
        db.close()


# –ù–∞—á–∞–ª–æ –±–ª–æ–∫–∞ —Ä–µ—à–µ–Ω–∏—è/–æ—Ç–≤–µ—Ç–∞ (—Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –º–µ–∂–¥—É –±—É–∫–≤–∞–º–∏ –ø–æ—Å–ª–µ OCR)
RE_SOLUTION_START = re.compile(
    r"^\s*–†\s*–µ\s*—à\s*–µ\s*–Ω\s*–∏\s*–µ\s*\.|^\s*–†–µ—à–µ–Ω–∏–µ\s*\."
    r"|^\s*–û\s*—Ç\s*–≤\s*–µ\s*—Ç\s*\.|^\s*–û—Ç–≤–µ—Ç\s*\.",
    re.IGNORECASE
)


def segment_problems(text: str, page_num: int) -> list[dict]:
    """
    –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –∑–∞–¥–∞—á–∏ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —Ä–∞–∑–º–µ—Ç–∫–∏ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ —É—á–µ–±–Ω–∏–∫–∞:
    –∑–∞–¥–∞—á–∞, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ, –∑–∞–¥–∞–Ω–∏–µ, –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–µ/–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ, –≤–æ–ø—Ä–æ—Å, –ø–∞—Ä–∞–≥—Ä–∞—Ñ, ¬ß, N. / N) –∏ —Ç.–¥.
    –ì—Ä–∞–Ω–∏—Ü–∞ —É—Å–ª–æ–≤–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è: —Å—Ç—Ä–æ–∫–∞ ¬´–†–µ—à–µ–Ω–∏–µ.¬ª ‚Äî —É—Å–ª–æ–≤–∏–µ –¥–æ –Ω–µ—ë, —Ä–µ—à–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–µ—ë –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏.
    
    Returns list of dicts: number, text (—É—Å–ª–æ–≤–∏–µ), solution_text (–µ—Å–ª–∏ –µ—Å—Ç—å), confidence
    """
    if not text or len(text.strip()) < 10:
        return []
    
    problems = []
    # –ë–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã ‚Äî —Ä–∞–Ω—å—à–µ. –ù–æ–º–µ—Ä –≤—Å–µ–≥–¥–∞ –≤ –≥—Ä—É–ø–ø–µ 1.
    patterns = [
        r"–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ\s*(?:‚Ññ\s*)?(?:\(\s*)?(\d+)(?:\))?",
        r"–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è\s*(?:‚Ññ\s*)?(?:\(\s*)?(\d+)(?:\))?",
        r"–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ\s*(?:‚Ññ\s*)?(?:\(\s*)?(\d+)(?:\))?",
        r"–ó–∞–¥–∞—á–∞\s*\(\s*(\d+)\s*\)\s*\.?",   # –ó–∞–¥–∞—á–∞ (22).
        r"–ó–∞–¥–∞—á–∞\s+(\d+)",
        r"–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ\s+(\d+)",
        r"–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ\s*\(\s*(\d+)\s*\)",
        r"–í–æ–ø—Ä–æ—Å\s*(?:‚Ññ\s*)?(?:\(\s*)?(\d+)(?:\))?",
        r"–í–æ–ø—Ä–æ—Å—ã?\s+(?:–∫?\s*)?(?:‚Ññ\s*)?(\d+)",
        r"–ó–∞–¥–∞–Ω–∏–µ\s*\(\s*(\d+)\s*\)",
        r"–ó–∞–¥–∞–Ω–∏–µ\s+(\d+)",
        r"–ó–∞–¥–∞–Ω–∏–µ\s*(?:‚Ññ\s*)?(\d+)",
        r"¬ß\s*(\d+(?:\.\d+)?)",
        r"–ü–∞—Ä–∞–≥—Ä–∞—Ñ\s*(\d+)",
        r"Exercise\s+(\d+)",   # –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —É—á–µ–±–Ω–∏–∫–∏
        r"‚Ññ\s*(\d+(?:\.\d+)?)",
        r"^(\d+)\.\s+",        # 1. –¢–µ–∫—Å—Ç
        r"^(\d+)\)\s+",       # 1) –¢–µ–∫—Å—Ç
    ]
    
    lines = text.split("\n")
    current_problem = None
    current_number = None
    solution_lines = None  # —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Ä–µ—à–µ–Ω–∏—è –ø–æ—Å–ª–µ ¬´–†–µ—à–µ–Ω–∏–µ.¬ª

    for line in lines:
        stripped = line.strip()
        if not stripped:
            if current_problem and solution_lines is None:
                current_problem += "\n" + line.rstrip()
            elif solution_lines is not None:
                solution_lines.append(line.rstrip())
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞—á–∞–ª–æ –±–ª–æ–∫–∞ ¬´–†–µ—à–µ–Ω–∏–µ.¬ª
        if RE_SOLUTION_START.search(stripped):
            if current_problem and len(current_problem) > 20:
                problems.append({
                    "number": current_number,
                    "text": current_problem.strip(),
                    "solution_text": None,
                    "confidence": 60,
                })
            current_problem = None
            current_number = None
            solution_lines = [stripped]
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏
        is_problem_start = False
        number = None
        for pattern in patterns:
            match = re.search(pattern, stripped, re.IGNORECASE)
            if match:
                is_problem_start = True
                number = match.group(1)
                break

        if is_problem_start:
            # –ü—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–µ
            if solution_lines is not None and problems:
                sol = "\n".join(s for s in solution_lines if s).strip()
                if sol:
                    problems[-1]["solution_text"] = sol
            solution_lines = None

            if current_problem and len(current_problem) > 20:
                problems.append({
                    "number": current_number,
                    "text": current_problem.strip(),
                    "solution_text": None,
                    "confidence": 60,
                })
            current_problem = stripped
            current_number = number
        elif solution_lines is not None:
            solution_lines.append(stripped)
        elif current_problem is not None:
            current_problem += "\n" + stripped

    if solution_lines is not None and problems:
        sol = "\n".join(s for s in solution_lines if s).strip()
        if sol:
            problems[-1]["solution_text"] = sol

    if current_problem and len(current_problem) > 20:
        problems.append({
            "number": current_number,
            "text": current_problem.strip(),
            "solution_text": None,
            "confidence": 60,
        })

    return problems


# –ì—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞: –ø–æ—Å–ª–µ —Ç–µ–æ—Ä–∏–∏ –∏–¥—ë—Ç –±–ª–æ–∫ –∑–∞–¥–∞–Ω–∏–π
RE_SECTION_HEADER = re.compile(r"^\s*[¬ß\$]\s*(\d+)[.,\s]|^\s*–ü–∞—Ä–∞–≥—Ä–∞—Ñ\s*(\d+)[.,\s]", re.IGNORECASE)
RE_TASK_BLOCK_START = re.compile(
    r"^\s*(?:–ó–∞–¥–∞—á–∏|–£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è|–í–æ–ø—Ä–æ—Å—ã\s+–∫\s+–ø–∞—Ä–∞–≥—Ä–∞—Ñ—É|–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ\s+–∑–∞–¥–∞–Ω–∏—è|–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ\s+–∑–∞–¥–∞–Ω–∏—è)\s*[.:]?",
    re.IGNORECASE
)


def extract_and_save_section_theory(db, book_id: int, pdf_source_id: int) -> Optional[int]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º (¬ß N) –∏–∑ ocr_text —Å—Ç—Ä–∞–Ω–∏—Ü
    –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ section_theory. –ù—É–∂–Ω–æ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
    –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ LLM.
    """
    from models import PdfPage, SectionTheory

    pages = (
        db.query(PdfPage)
        .filter(PdfPage.pdf_source_id == pdf_source_id)
        .filter(PdfPage.ocr_text != None)
        .filter(PdfPage.ocr_text != "")
        .order_by(PdfPage.page_num)
        .all()
    )
    if not pages:
        return None

    sections = []  # list of (section_label, text, start_page, end_page)
    current_section = None
    current_text = []
    current_start_page = None
    current_end_page = None

    def flush_section():
        nonlocal current_section, current_text, current_start_page, current_end_page
        if current_section is not None and current_text:
            text = "\n".join(current_text).strip()
            if len(text) > 50:
                end = current_end_page if current_end_page is not None else current_start_page
                sections.append((current_section, text, current_start_page, end))
        current_section = None
        current_text = []
        current_start_page = None
        current_end_page = None

    for i, page in enumerate(pages):
        lines = (page.ocr_text or "").split("\n")

        for line in lines:
            stripped = line.strip()
            # –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            sec_match = RE_SECTION_HEADER.search(stripped)
            if sec_match:
                num = sec_match.group(1) or sec_match.group(2)
                flush_section()
                current_section = f"¬ß{num}"
                current_start_page = page.page_num
                current_end_page = page.page_num
                current_text = [stripped]
                continue

            # –ù–∞—á–∞–ª–æ –±–ª–æ–∫–∞ –∑–∞–¥–∞–Ω–∏–π ‚Äî –≥—Ä–∞–Ω–∏—Ü–∞ —Ç–µ–æ—Ä–∏–∏
            if RE_TASK_BLOCK_START.search(stripped):
                flush_section()
                continue

            if current_section is not None:
                current_text.append(stripped if stripped else line.rstrip())

        if current_section is not None:
            current_end_page = page.page_num

    flush_section()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–ª–æ–∫–∏ —Å –æ–¥–Ω–∏–º –∏ —Ç–µ–º –∂–µ ¬ß (–æ–¥–∏–Ω –ø–∞—Ä–∞–≥—Ä–∞—Ñ –º–æ–∂–µ—Ç –≤—Å–ø–ª—ã–≤–∞—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö)
    merged = {}
    for section_label, theory_text, start_page, end_page in sections:
        if section_label not in merged:
            merged[section_label] = {"texts": [], "start": start_page, "end": end_page}
        merged[section_label]["texts"].append(theory_text)
        merged[section_label]["start"] = min(merged[section_label]["start"], start_page)
        merged[section_label]["end"] = max(merged[section_label]["end"], end_page)

    saved = 0
    for section_label, data in merged.items():
        theory_text = "\n\n".join(data["texts"]).strip()
        if len(theory_text) < 50:
            continue
        start_page = data["start"]
        end_page = data["end"]
        page_ref = f"—Å—Ç—Ä. {start_page + 1}" if start_page == end_page else f"—Å—Ç—Ä. {start_page + 1}‚Äì{end_page + 1}"
        existing = db.query(SectionTheory).filter(
            SectionTheory.book_id == book_id,
            SectionTheory.section == section_label,
        ).first()
        if existing:
            existing.theory_text = theory_text
            existing.page_ref = page_ref
            existing.updated_at = datetime.utcnow()
        else:
            db.add(SectionTheory(
                book_id=book_id,
                section=section_label,
                theory_text=theory_text,
                page_ref=page_ref,
            ))
        saved += 1

    if saved:
        db.commit()
    return saved if merged else None


def run_llm_normalize_only(pdf_source_id: int) -> dict:
    """
    –¢–æ–ª—å–∫–æ LLM-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª,
    –ø—Ä–æ–≥–Ω–∞—Ç—å —á–µ—Ä–µ–∑ OpenAI (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª/OCR), –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Ñ–∞–π–ª –∏ –ø–µ—Ä–µ–∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ –ë–î.
    –ù–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç OCR ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª —É–∂–µ –µ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞).
    """
    from models import PdfSource, Book

    db = SessionLocal()
    try:
        pdf_source = db.query(PdfSource).filter(PdfSource.id == pdf_source_id).first()
        if not pdf_source:
            return {"status": "error", "message": f"PdfSource {pdf_source_id} not found"}
        book_id = pdf_source.book_id
        book = db.query(Book).filter(Book.id == book_id).first()
        subject = (book.subject if book else "geometry") or "geometry"
    finally:
        db.close()

    if not HAS_OCR_FILES:
        return {"status": "error", "message": "ocr_files module not available"}

    pages_data = read_normalized_pages(book_id, pdf_source_id)
    if not pages_data:
        return {"status": "error", "message": "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ OCR (–ù–∞—á–∞—Ç—å OCR)."}

    page_texts = [t for _, t in sorted(pages_data, key=lambda x: x[0])]
    total = len(page_texts)
    print(f"   üìÑ LLM-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {pdf_source_id}: {total} —Å—Ç—Ä–∞–Ω–∏—Ü (–±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ OCR)")

    checkpoint_path = get_llm_checkpoint_path(book_id, pdf_source_id)
    redis_conn = None
    try:
        from redis import Redis
        redis_conn = Redis.from_url(settings.redis_url)
    except Exception:
        pass
    progress_key = f"llm_norm_progress:{pdf_source_id}"

    def progress_callback(current: int, total_pages: int) -> None:
        if redis_conn:
            try:
                redis_conn.setex(progress_key, 3600, f"{current}/{total_pages}")
            except Exception:
                pass

    cancel_key = f"cancel_llm:{pdf_source_id}"

    def cancel_check() -> bool:
        if not redis_conn:
            return False
        try:
            if redis_conn.get(cancel_key):
                redis_conn.delete(cancel_key)
                return True
        except Exception:
            pass
        return False

    try:
        from llm_ocr_correct import correct_normalized_pages, LLMCancelRequested
        if redis_conn:
            try:
                redis_conn.setex(progress_key, 3600, f"0/{total}")
            except Exception:
                pass
        corrected = correct_normalized_pages(
            page_texts,
            subject=subject,
            checkpoint_path=checkpoint_path,
            progress_callback=progress_callback,
            cancel_check=cancel_check,
        )
    except LLMCancelRequested:
        if redis_conn:
            try:
                redis_conn.delete(progress_key)
            except Exception:
                pass
        return {"status": "cancelled", "message": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ú–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º –Ω–∞–∂–∞—Ç–∏–µ–º ¬´LLM –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è¬ª."}
    except Exception as e:
        return {"status": "error", "message": str(e)}

    write_normalized_md(book_id, pdf_source_id, corrected)
    print(f"   üìÅ –§–∞–π–ª –æ–±–Ω–æ–≤–ª—ë–Ω, –ø–µ—Ä–µ–∏–º–ø–æ—Ä—Ç –≤ –ë–î...")
    out = import_from_normalized_file(pdf_source_id)
    if redis_conn:
        try:
            redis_conn.delete(progress_key)
        except Exception:
            pass
    return out


def import_from_normalized_file(pdf_source_id: int) -> dict:
    """
    –ò–º–ø–æ—Ä—Ç –≤ –ë–î —Ç–æ–ª—å–∫–æ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–±–µ–∑ OCR).
    –ß–∏—Ç–∞–µ—Ç data/ocr_normalized/{book_id}/{pdf_source_id}.md, –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∑–∞–ø–æ–ª–Ω—è–µ—Ç
    pdf_pages.ocr_text, –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∑–∞–¥–∞—á –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏.
    """
    from models import PdfSource, PdfPage, Problem

    if not HAS_OCR_FILES:
        return {"status": "error", "message": "ocr_files module not available"}

    db = SessionLocal()
    try:
        pdf_source = db.query(PdfSource).filter(PdfSource.id == pdf_source_id).first()
        if not pdf_source:
            return {"status": "error", "message": f"PdfSource {pdf_source_id} not found"}

        path = get_ocr_normalized_path(pdf_source.book_id, pdf_source_id)
        if not path.exists():
            return {"status": "error", "message": f"Normalized file not found: {path}"}

        pages_data = read_normalized_pages(pdf_source.book_id, pdf_source_id)
        if not pages_data:
            return {"status": "error", "message": "No pages in normalized file or parse error"}

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∑–∞–¥–∞—á–∏ —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        existing_pages = db.query(PdfPage).filter(PdfPage.pdf_source_id == pdf_source_id).all()
        for p in existing_pages:
            db.query(Problem).filter(Problem.source_page_id == p.id).delete()
        db.query(PdfPage).filter(PdfPage.pdf_source_id == pdf_source_id).delete()
        db.flush()

        book_id = pdf_source.book_id
        problems_found = 0
        for page_num_1based, text in pages_data:
            page_num = page_num_1based - 1  # –≤ –ë–î page_num 0-based
            pdf_page = PdfPage(
                pdf_source_id=pdf_source_id,
                page_num=page_num,
                ocr_text=text,
                ocr_confidence=70,
            )
            db.add(pdf_page)
            db.flush()
            problems = segment_problems(text, page_num)
            for prob in problems:
                problem = Problem(
                    book_id=book_id,
                    source_page_id=pdf_page.id,
                    number=prob.get("number"),
                    section=prob.get("section"),
                    problem_text=prob["text"],
                    solution_text=prob.get("solution_text"),
                    page_ref=f"—Å—Ç—Ä. {page_num + 1}",
                    confidence=prob.get("confidence", 50),
                )
                db.add(problem)
                problems_found += 1

        theory_count = extract_and_save_section_theory(db, book_id, pdf_source_id)
        pdf_source.status = "done"
        db.commit()
        print(f"   ‚úÖ Import from normalized file: {len(pages_data)} pages, {problems_found} problems")
        return {
            "status": "success",
            "pdf_source_id": pdf_source_id,
            "pages_imported": len(pages_data),
            "problems_found": problems_found,
            "section_theory_saved": theory_count,
        }
    except Exception as e:
        db.rollback()
        return {"status": "error", "message": str(e)}
    finally:
        db.close()


def import_from_normalized_file_llm(pdf_source_id: int) -> dict:
    """
    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ë–î –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—à–µ–Ω–∏–π LLM (—Å–º. docs/LLM_DISTRIBUTION_DESIGN.md).
    –ß–∏—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π .md ‚Üí –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –±–ª–æ–∫–æ–≤ ‚Üí LLM –±–∞—Ç—á–∞–º–∏ ‚Üí –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å
    –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É: pdf_pages, problems, section_theory (–ø–æ –∫–Ω–∏–≥–µ).
    """
    from models import PdfSource, PdfPage, Problem, ProblemPart, SectionTheory

    if not HAS_OCR_FILES:
        return {"status": "error", "message": "ocr_files module not available"}

    db = SessionLocal()
    try:
        pdf_source = db.query(PdfSource).filter(PdfSource.id == pdf_source_id).first()
        if not pdf_source:
            return {"status": "error", "message": f"PdfSource {pdf_source_id} not found"}
        book_id = pdf_source.book_id
        from models import Book
        book = db.query(Book).filter(Book.id == book_id).first()
        subject = (book.subject if book else "geometry") or "geometry"

        path = get_ocr_normalized_path(book_id, pdf_source_id)
        if not path.exists():
            return {"status": "error", "message": f"Normalized file not found: {path}"}

        pages_data = read_normalized_pages(book_id, pdf_source_id)
        if not pages_data:
            return {"status": "error", "message": "No pages in normalized file or parse error"}

        def progress(batch_idx: int, total: int) -> None:
            print(f"   üì¶ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ LLM: –±–∞—Ç—á {batch_idx}/{total}")

        redis_conn = None
        try:
            from redis import Redis
            redis_conn = Redis.from_url(settings.redis_url)
        except Exception:
            pass
        cancel_key = f"cancel_import_db:{pdf_source_id}"

        def cancel_check() -> bool:
            if not redis_conn:
                return False
            try:
                if redis_conn.get(cancel_key):
                    redis_conn.delete(cancel_key)
                    return True
            except Exception:
                pass
            return False

        from llm_distribute import distribute_batches, ImportDBCancelRequested
        try:
            parsed = distribute_batches(pages_data, subject, progress_callback=progress, cancel_check=cancel_check)
        except ImportDBCancelRequested:
            return {"status": "cancelled", "message": "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ë–î –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º."}
        if not parsed:
            return {"status": "error", "message": "LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –±–ª–æ–∫–∏ (–ø—Ä–æ–≤–µ—Ä—å OPENAI_API_KEY –∏ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞)"}

        # –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å: —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É –∏ —Ç–µ–æ—Ä–∏—é –ø–æ –∫–Ω–∏–≥–µ
        existing_pages = db.query(PdfPage).filter(PdfPage.pdf_source_id == pdf_source_id).all()
        for p in existing_pages:
            db.query(Problem).filter(Problem.source_page_id == p.id).delete()
        db.query(PdfPage).filter(PdfPage.pdf_source_id == pdf_source_id).delete()
        db.query(SectionTheory).filter(SectionTheory.book_id == book_id).delete()
        db.flush()

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pdf_pages –∏–∑ —Ñ–∞–π–ª–∞ (ocr_text –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º)
        page_num_to_id: dict[int, int] = {}
        for page_num_1based, text in pages_data:
            page_num = page_num_1based - 1
            pdf_page = PdfPage(
                pdf_source_id=pdf_source_id,
                page_num=page_num,
                ocr_text=text,
                ocr_confidence=70,
            )
            db.add(pdf_page)
            db.flush()
            page_num_to_id[page_num_1based] = pdf_page.id

        # –¢–µ–æ—Ä–∏—è: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –±–ª–æ–∫–∏ –ø–æ section
        theory_by_section: dict[str, list[str]] = {}
        for b in parsed:
            t = (b.get("type") or "").lower()
            if t not in ("section_theory", "theory"):
                continue
            sec = (b.get("section") or "").strip() or None
            if not sec:
                continue
            theory_text = (b.get("theory_text") or "").strip()
            if not theory_text:
                continue
            if not sec.startswith("¬ß"):
                sec = f"¬ß{sec.lstrip()}"
            if sec not in theory_by_section:
                theory_by_section[sec] = []
            theory_by_section[sec].append(theory_text)

        for section_label, texts in theory_by_section.items():
            theory_text = "\n\n".join(texts).strip()
            if len(theory_text) < 30:
                continue
            db.add(SectionTheory(book_id=book_id, section=section_label, theory_text=theory_text, page_ref=None))

        # –ó–∞–¥–∞—á–∏ –∏–∑ –±–ª–æ–∫–æ–≤ type=problem; type=solution_only –ø—Ä–∏–∫—Ä–µ–ø–ª—è–µ—Ç—Å—è –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–¥–∞—á–µ
        problems_found = 0
        last_added_problem = None
        for b in parsed:
            t = (b.get("type") or "").lower()
            if t == "solution_only":
                sol = (b.get("solution_text") or "").strip()
                ans = (b.get("answer_text") or "").strip()
                if last_added_problem and (sol or ans):
                    if sol:
                        last_added_problem.solution_text = sol if not last_added_problem.solution_text else (last_added_problem.solution_text + "\n\n" + sol)
                    if ans and not last_added_problem.answer_text:
                        last_added_problem.answer_text = ans
                continue
            if t != "problem":
                continue
            problem_text = (b.get("problem_text") or "").strip()
            if not problem_text:
                continue
            page_num_1 = b.get("_page_num") or 1
            source_page_id = page_num_to_id.get(page_num_1)
            if not source_page_id:
                source_page_id = next(iter(page_num_to_id.values()), None)
            sec = (b.get("section") or "").strip() or None
            if sec and not sec.startswith("¬ß"):
                sec = f"¬ß{sec.lstrip()}"
            parts_raw = b.get("parts")
            has_parts = isinstance(parts_raw, list) and len(parts_raw) > 0
            problem = Problem(
                book_id=book_id,
                source_page_id=source_page_id,
                number=b.get("number"),
                section=sec,
                problem_text=problem_text,
                solution_text=(b.get("solution_text") or "").strip() or None,
                answer_text=(b.get("answer_text") or "").strip() or None,
                page_ref=f"—Å—Ç—Ä. {page_num_1}" if page_num_1 else None,
                confidence=70,
                has_parts=has_parts,
            )
            db.add(problem)
            db.flush()
            last_added_problem = problem
            if has_parts:
                for part in parts_raw:
                    if not isinstance(part, dict):
                        continue
                    part_number = (part.get("part_number") or "").strip() or None
                    part_text = (part.get("part_text") or "").strip() or None
                    if part_number is None and part_text is None:
                        continue
                    db.add(ProblemPart(
                        problem_id=problem.id,
                        part_number=part_number or "?",
                        part_text=part_text,
                        answer_text=(part.get("answer_text") or "").strip() or None,
                        solution_text=(part.get("solution_text") or "").strip() or None,
                    ))
            problems_found += 1

        # –ë–ª–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∫–æ–Ω—Ü–µ –∫–Ω–∏–≥–∏: —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ –Ω–æ–º–µ—Ä—É –∑–∞–¥–∞—á–∏ –∏ –ø–∏—à–µ–º answer_text –≤ –ë–î
        for b in parsed:
            if (b.get("type") or "").lower() != "answers_block":
                continue
            answers_list = b.get("answers")
            if not isinstance(answers_list, list):
                continue
            for item in answers_list:
                if not isinstance(item, dict):
                    continue
                num = (item.get("number") or "").strip() or None
                ans = (item.get("answer_text") or "").strip() or None
                if not num or not ans:
                    continue
                prob = db.query(Problem).filter(Problem.book_id == book_id, Problem.number == num).first()
                if prob and not prob.answer_text:
                    prob.answer_text = ans

        pdf_source.status = "done"
        db.commit()
        theory_count = len(theory_by_section)
        print(f"   ‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ LLM: {len(pages_data)} —Å—Ç—Ä–∞–Ω–∏—Ü, {problems_found} –∑–∞–¥–∞—á, {theory_count} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —Ç–µ–æ—Ä–∏–∏")
        return {
            "status": "success",
            "pdf_source_id": pdf_source_id,
            "pages_imported": len(pages_data),
            "problems_found": problems_found,
            "section_theory_saved": theory_count,
        }
    except Exception as e:
        db.rollback()
        return {"status": "error", "message": str(e)}
    finally:
        db.close()


def reanalyze_pdf_source(pdf_source_id: int) -> dict:
    """
    –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: –ø–æ ocr_text –∏–∑ pdf_pages
    –∑–∞–Ω–æ–≤–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è segment_problems –∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è problems.
    PDF –Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è, ocr_text –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è.
    """
    from models import PdfSource, PdfPage, Problem

    db = SessionLocal()
    try:
        pdf_source = db.query(PdfSource).filter(PdfSource.id == pdf_source_id).first()
        if not pdf_source:
            return {"status": "error", "message": f"PdfSource {pdf_source_id} not found"}

        pages = (
            db.query(PdfPage)
            .filter(PdfPage.pdf_source_id == pdf_source_id, PdfPage.ocr_text != None)
            .filter(PdfPage.ocr_text != "")
            .order_by(PdfPage.page_num)
            .all()
        )
        if not pages:
            return {"status": "skipped", "message": "No pages with ocr_text", "problems_found": 0}

        total_problems = 0
        for i, page in enumerate(pages):
            # –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –∑–∞–¥–∞—á–∏ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            db.query(Problem).filter(Problem.source_page_id == page.id).delete()
            # –ó–∞–Ω–æ–≤–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
            problems = segment_problems(page.ocr_text or "", page.page_num)
            for prob in problems:
                problem = Problem(
                    book_id=pdf_source.book_id,
                    source_page_id=page.id,
                    number=prob.get("number"),
                    section=prob.get("section"),
                    problem_text=prob["text"],
                    solution_text=prob.get("solution_text"),
                    page_ref=f"—Å—Ç—Ä. {page.page_num + 1}",
                    confidence=prob.get("confidence", 50),
                )
                db.add(problem)
                total_problems += 1
            if (i + 1) % 50 == 0:
                db.commit()
                print(f"   üìÉ Reanalyzed {i + 1}/{len(pages)} pages, {total_problems} problems")

        # –û–±–Ω–æ–≤–∏—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
        theory_count = extract_and_save_section_theory(db, pdf_source.book_id, pdf_source_id)
        if theory_count is not None:
            print(f"   üìñ Section theory: {theory_count} paragraphs updated")

        db.commit()
        print(f"   ‚úÖ Reanalyze done: {len(pages)} pages, {total_problems} problems")
        return {
            "status": "success",
            "pdf_source_id": pdf_source_id,
            "pages_reanalyzed": len(pages),
            "problems_found": total_problems,
            "section_theory_saved": theory_count,
        }
    except Exception as e:
        db.rollback()
        return {"status": "error", "message": str(e)}
    finally:
        db.close()


# ===========================================
# Queue Integration
# ===========================================

def enqueue_ingestion(pdf_source_id: int) -> str:
    """Enqueue PDF ingestion job."""
    from redis import Redis
    from rq import Queue

    redis_conn = Redis.from_url(settings.redis_url)
    queue = Queue("ingestion", connection=redis_conn)

    job = queue.enqueue(
        process_pdf_source,
        pdf_source_id,
        job_timeout="30m",  # PDF processing can be slow
        result_ttl=3600,
    )

    return job.id


def enqueue_reanalyze(pdf_source_id: int) -> str:
    """–ü–æ—Å—Ç–∞–≤–∏—Ç—å –≤ –æ—á–µ—Ä–µ–¥—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–±–µ–∑ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞–Ω–∏—è PDF)."""
    from redis import Redis
    from rq import Queue

    redis_conn = Redis.from_url(settings.redis_url)
    queue = Queue("ingestion", connection=redis_conn)

    job = queue.enqueue(
        reanalyze_pdf_source,
        pdf_source_id,
        job_timeout="15m",
        result_ttl=3600,
    )
    return job.id


def enqueue_import_from_normalized_file(pdf_source_id: int) -> str:
    """–ü–æ—Å—Ç–∞–≤–∏—Ç—å –≤ –æ—á–µ—Ä–µ–¥—å –∏–º–ø–æ—Ä—Ç –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–±–µ–∑ OCR)."""
    from redis import Redis
    from rq import Queue

    redis_conn = Redis.from_url(settings.redis_url)
    queue = Queue("ingestion", connection=redis_conn)
    job = queue.enqueue(
        import_from_normalized_file,
        pdf_source_id,
        job_timeout="15m",
        result_ttl=3600,
    )
    return job.id


def process_all_pending() -> list[dict]:
    """Process all pending PDF sources."""
    from models import PdfSource
    
    db = SessionLocal()
    try:
        pending = db.query(PdfSource).filter(PdfSource.status == "pending").all()
        print(f"üìö Found {len(pending)} pending PDF sources")
        
        results = []
        for pdf_source in pending:
            result = process_pdf_source(pdf_source.id)
            results.append(result)
        
        return results
    finally:
        db.close()


# ===========================================
# CLI
# ===========================================

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        pdf_source_id = int(sys.argv[1])
        result = process_pdf_source(pdf_source_id)
    else:
        results = process_all_pending()
        print(f"\nüìä Processed {len(results)} PDF sources")
