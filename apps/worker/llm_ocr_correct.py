"""
–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ OCR —á–µ—Ä–µ–∑ OpenAI: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É, –ø—Ä–∏–≥–æ–¥–Ω–æ–º—É –¥–ª—è –ë–î, —á–∞—Ç–∞ –∏ —Å–∫—Ä–∏–ø—Ç–æ–≤.

–ë–µ–∑ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –∑–∞–º–µ–Ω ‚Äî –º–æ–¥–µ–ª—å –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–ø—Ä–µ–¥–º–µ—Ç, —É—á–µ–±–Ω–∏–∫).
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: –ø—Ä–∏ —Å–±–æ–µ –º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (–±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ API).
"""

import json
import os
import re
from pathlib import Path
from typing import Callable, List, Optional

# –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ –±–ª–æ–∫–∞–º ## –°—Ç—Ä–∞–Ω–∏—Ü–∞ N
PAGE_HEADER = re.compile(r"^##\s+–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+(\d+)\s*$", re.IGNORECASE)

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL_TEXT", "gpt-4o")

SYSTEM_PROMPT = """–¢—ã –∏—Å–ø—Ä–∞–≤–ª—è–µ—à—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ OCR —É—á–µ–±–Ω–∏–∫–∞. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ –Ω–µ—É–¥–æ–±–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —Ñ–æ—Ä–º—É–ª.

–ó–ê–î–ê–ß–ò:
1. –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫–∏ OCR: –ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤–º–µ—Å—Ç–æ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã (TEOPEMA ‚Üí –¢–ï–û–†–ï–ú–ê, CHHYCOB ‚Üí –°–ò–ù–£–°–û–í), –ø–µ—Ä–µ–ø—É—Ç–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (6‚Üîb, ?‚Üî¬≤), —Å–∫–ª–µ–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞.
2. –ü—Ä–∏–≤–µ—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¢–û–õ–¨–ö–û —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –æ–±—ã—á–Ω–æ–º —Ç–µ–∫—Å—Ç–µ, –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–∞—Ö –∏ –≤ –ë–î: Unicode (¬≤, ¬≥, ‚àö, ‚à†, ¬∞, √ó, √∑, œÄ, ‚â§, ‚â•, ‚â†, ‚àû, ¬±, ‚âà) –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ^ –¥–ª—è —Å—Ç–µ–ø–µ–Ω–∏ (x^2).
   - –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LaTeX —Å –æ–±—Ä–∞—Ç–Ω—ã–º —Å–ª—ç—à–µ–º (\\frac, \\sqrt –∏ —Ç.–ø.).
   - –î—Ä–æ–±–∏: –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞–∫ a/b –∏–ª–∏ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —Å √∑.
   - –§–æ—Ä–º—É–ª—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω—ã –∏ —á–µ–ª–æ–≤–µ–∫—É, –∏ —Å–∫—Ä–∏–ø—Ç–∞–º/LLM –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞.
3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: –∫–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞ "## –°—Ç—Ä–∞–Ω–∏—Ü–∞ N" –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –ü–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –º–µ–Ω—è—Ç—å.
4. –ö–æ–Ω—Ç–µ–∫—Å—Ç: —É—á–µ–±–Ω–∏–∫ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—Ä–µ–¥–º–µ—Ç—É ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ñ–æ—Ä–º—É–ª –∏ —Ç–µ—Ä–º–∏–Ω–æ–≤.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: –≤–µ—Ä–Ω–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —Ç–æ–º –∂–µ –≤–∏–¥–µ ‚Äî –±–ª–æ–∫–∏ "## –°—Ç—Ä–∞–Ω–∏—Ü–∞ 1", "## –°—Ç—Ä–∞–Ω–∏—Ü–∞ 2", ... —Å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Ç–µ–∫—Å—Ç–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –ù–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ –±–ª–æ–∫–æ–≤."""


def _parse_pages_from_response(content: str) -> List[tuple[int, str]]:
    """–†–∞–∑–æ–±—Ä–∞—Ç—å –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ –±–ª–æ–∫–∞–º ## –°—Ç—Ä–∞–Ω–∏—Ü–∞ N. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç [(page_num, text), ...]."""
    result = []
    current_page = None
    current_lines = []
    for line in content.split("\n"):
        match = PAGE_HEADER.match(line.strip())
        if match:
            if current_page is not None:
                result.append((current_page, "\n".join(current_lines).strip()))
            current_page = int(match.group(1))
            current_lines = []
            continue
        if current_page is not None:
            current_lines.append(line)
    if current_page is not None:
        result.append((current_page, "\n".join(current_lines).strip()))
    return result


def _build_batch_chunk(page_texts: List[str], start_index: int) -> str:
    """–°–æ–±—Ä–∞—Ç—å –æ–¥–∏–Ω –±–∞—Ç—á –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏: ## –°—Ç—Ä–∞–Ω–∏—Ü–∞ (start+1) ... ## –°—Ç—Ä–∞–Ω–∏—Ü–∞ (start+len)."""
    lines = []
    for i, text in enumerate(page_texts):
        page_num = start_index + i + 1
        lines.append(f"## –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
        lines.append("")
        lines.append((text or "").strip())
        lines.append("")
    return "\n".join(lines).rstrip()


def _load_checkpoint(path: Path, total_pages: int, page_texts: List[str]) -> tuple[List[str], set[int]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞; –≤–µ—Ä–Ω—É—Ç—å (result, done_indices)."""
    result: List[str] = [""] * total_pages
    done_indices: set[int] = set()
    if not path.exists():
        return result, done_indices
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        done_list = data.get("done", [])
        done_indices = set(int(x) for x in done_list if 0 <= int(x) < total_pages)
        for i in range(total_pages):
            if str(i) in data and data[str(i)]:
                result[i] = data[str(i)]
            elif i not in done_indices:
                result[i] = page_texts[i] if i < len(page_texts) else ""
    except Exception:
        return [page_texts[i] if i < len(page_texts) else "" for i in range(total_pages)], set()
    for i in range(total_pages):
        if not result[i]:
            result[i] = page_texts[i] if i < len(page_texts) else ""
    return result, done_indices


def _save_checkpoint(path: Path, result: List[str], done_indices: set[int]) -> None:
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç (–ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ) –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ —Å–±–æ—è."""
    path.parent.mkdir(parents=True, exist_ok=True)
    data = {"done": sorted(done_indices), **{str(i): result[i] for i in done_indices if i < len(result)}}
    path.write_text(json.dumps(data, ensure_ascii=False, indent=None), encoding="utf-8")


def correct_normalized_pages(
    page_texts: List[str],
    subject: str = "geometry",
    batch_size: int = 10,
    model: Optional[str] = None,
    checkpoint_path: Optional[Path] = None,
    progress_callback: Optional[Callable[[int, int], None]] = None,
) -> List[str]:
    """
    –ü—Ä–æ–≥–Ω–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ OpenAI –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è OCR –∏ —Ñ–æ—Ä–º—É–ª.

    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ —Å–∏–º–≤–æ–ª—ã –≤ —Ñ–æ—Ä–º—É–ª–∞—Ö –∑–∞–¥–∞–Ω—ã –≤ SYSTEM_PROMPT (Unicode + ^, –±–µ–∑ LaTeX).
    –ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ API –∫–ª—é—á–∞ –∏–ª–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.

    –ß–µ–∫–ø–æ–∏–Ω—Ç: –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω checkpoint_path, –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è.
    –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—É—Å–∫–µ —Å —Ç–µ–º –∂–µ –ø—É—Ç—ë–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –≤ API —Å–Ω–æ–≤–∞.

    Args:
        page_texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ—Å–ª–µ ocr_cleaner).
        subject: –ø—Ä–µ–¥–º–µ—Ç (geometry, math, physics, ...) –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        batch_size: —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ.
        model: –º–æ–¥–µ–ª—å OpenAI (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ env –∏–ª–∏ gpt-4o).
        checkpoint_path: –ø—É—Ç—å –∫ JSON-—á–µ–∫–ø–æ–∏–Ω—Ç—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ —Å–±–æ—è.
        progress_callback: –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ —Å (current, total).

    Returns:
        –°–ø–∏—Å–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã.
    """
    if not page_texts:
        return page_texts
    if not OPENAI_API_KEY:
        print("   ‚ö†Ô∏è  OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω, –ø—Ä–æ–ø—É—Å–∫ LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ñ–æ—Ä–º—É–ª")
        return page_texts

    try:
        from openai import OpenAI
    except ImportError:
        print("   ‚ö†Ô∏è  openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫ LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏")
        return page_texts

    client = OpenAI(api_key=OPENAI_API_KEY)
    model_name = model or os.environ.get("OPENAI_MODEL_TEXT", OPENAI_MODEL)
    total_pages = len(page_texts)
    result: List[str] = _load_checkpoint(checkpoint_path, total_pages, page_texts) if checkpoint_path else [""] * total_pages
    # –ó–∞–ø–æ–ª–Ω–∏—Ç—å –Ω–µ–∑–∞—á–µ–∫–ø–æ–∏–Ω—á–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –∏—Å—Ö–æ–¥–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è fallback
    for i in range(total_pages):
        if not result[i]:
            result[i] = page_texts[i] if i < len(page_texts) else ""

    resumed = sum(1 for i in range(total_pages) if result[i] and result[i] != (page_texts[i] if i < len(page_texts) else ""))
    if checkpoint_path and checkpoint_path.exists() and resumed > 0:
        print(f"   üìÇ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞: —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ ~{resumed} —Å—Ç—Ä–∞–Ω–∏—Ü")

    total_batches = (total_pages + batch_size - 1) // batch_size

    for batch_idx in range(total_batches):
        start = batch_idx * batch_size
        end = min(start + batch_size, total_pages)
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞—Ç—á, –µ—Å–ª–∏ –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É–∂–µ –µ—Å—Ç—å –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ (–ø–æ –ø—Ä–∏–∑–Ω–∞–∫—É ¬´–æ—Ç–≤–µ—Ç –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–µ —Ä–∞–≤–µ–Ω –∏—Å—Ö–æ–¥–Ω–∏–∫—É¬ª)
        if checkpoint_path:
            from llm_ocr_correct import _parse_pages_from_response
            already_done = all(
                result[i] and (result[i] != (page_texts[i] if i < len(page_texts) else ""))
                for i in range(start, end)
            )
            if already_done:
                if progress_callback:
                    progress_callback(end, total_pages)
                continue

        batch = page_texts[start:end]
        chunk = _build_batch_chunk(batch, start)
        if not chunk.strip():
            for i in range(start, end):
                result[i] = page_texts[i]
                done_indices.add(i)
            if checkpoint_path:
                _save_checkpoint(checkpoint_path, result, done_indices)
            if progress_callback:
                progress_callback(end, total_pages)
            continue

        user_content = f"–ü—Ä–µ–¥–º–µ—Ç: {subject}.\n\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–±–ª–æ–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü):\n\n{chunk}"

        try:
            resp = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_content},
                ],
                temperature=0.2,
            )
            answer = (resp.choices[0].message.content or "").strip()
            parsed = _parse_pages_from_response(answer)
            for page_num, text in parsed:
                idx = page_num - 1
                if 0 <= idx < len(result):
                    result[idx] = text
            for i in range(start, end):
                if not result[i] and i < len(page_texts):
                    result[i] = page_texts[i]
            for i in range(start, end):
                done_indices.add(i)
            if checkpoint_path:
                _save_checkpoint(checkpoint_path, result, done_indices)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –±–∞—Ç—á–∞ {batch_idx + 1}/{total_batches}: {e}")
            for i in range(start, end):
                result[i] = page_texts[i] if i < len(page_texts) else ""
            if checkpoint_path:
                _save_checkpoint(checkpoint_path, result, done_indices)

        if progress_callback:
            progress_callback(min(end, total_pages), total_pages)
        if (batch_idx + 1) % 5 == 0 or batch_idx == total_batches - 1:
            print(f"   ü§ñ LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è: {min(end, total_pages)}/{total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")

    for i in range(len(result)):
        if not result[i] and i < len(page_texts):
            result[i] = page_texts[i]

    if checkpoint_path and checkpoint_path.exists():
        try:
            checkpoint_path.unlink()
        except Exception:
            pass
    return result
